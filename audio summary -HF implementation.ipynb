{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQrgucqQ3Cmn"
   },
   "source": [
    "# Summary from an Audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO8r477A3Cmo"
   },
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjfkjfbI3Cmo"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFB4Y68s3Cmp"
   },
   "source": [
    "# Download denver_extract.mp3\n",
    "\n",
    "https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MolAZnCC3Cmo"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbBApnU73Cmo"
   },
   "outputs": [],
   "source": [
    "# New capability - connect this Colab to my Google Drive\n",
    "# See immediately below this for instructions to obtain denver_extract.mp3\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "audio_filename = \"/content/denver_extract.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjZrnGNv3Cmp"
   },
   "outputs": [],
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQwJVDFa3Cmp"
   },
   "outputs": [],
   "source": [
    "# Sign in to OpenAI using Secrets in Colab\n",
    "\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVOaFJgR3Cmp"
   },
   "outputs": [],
   "source": [
    "# Use the Whisper OpenAI model to convert the Audio to Text\n",
    "# If you'd prefer to use an Open Source model, check \"alternative implementation\" at the bottom of this file\n",
    "\n",
    "audio_file = open(audio_filename, \"rb\")\n",
    "transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs0SgewI3Cmp"
   },
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown and in results in PT-BR please.\"\n",
    "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNwkcOca3Cmp"
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCh_CjoI3Cmp"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVcSM-P_3Cmq"
   },
   "outputs": [],
   "source": [
    "response = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYckwG0R3Cmq"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdSGjNti3Cmq"
   },
   "source": [
    "## Alternative implementation\n",
    "\n",
    "**Download audio files at**:\n",
    "https://www.bbc.co.uk/programmes/p02pc9zn/episodes/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dgYwVv73Cmq"
   },
   "outputs": [],
   "source": [
    "AUDIO_MODEL = \"openai/whisper-medium\"\n",
    "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "speech_model.to('cuda')\n",
    "processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=speech_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch.float16,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lddXYrId0E5-"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\")\n",
    "audio_filename2 = \"/content/LearningEnglishConversations-20250408-TheEnglishWeSpeakDoItForThePlot.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h9B-Ek63Cmr"
   },
   "outputs": [],
   "source": [
    "# Use the Whisper OpenAI model to convert the Audio to Text\n",
    "result2 = pipe(audio_filename2, return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98VG3Fbm3Cmr"
   },
   "outputs": [],
   "source": [
    "transcription2 = result2[\"text\"]\n",
    "print(transcription2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTK7a9TIxyE5"
   },
   "outputs": [],
   "source": [
    "system_message2 = \"You are an assistant that produces notes from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown and in results in PT-BR please.\"\n",
    "user_prompt2 = f\"Below is an extract transcript of an audio file. Please write a in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription2}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message2},\n",
    "    {\"role\": \"user\", \"content\": user_prompt2}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z13a8Lhmx9T_"
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1JQekiPyBUj"
   },
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer2.pad_token = tokenizer2.eos_token\n",
    "inputs = tokenizer2.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer2)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUMjR1plyGSY"
   },
   "outputs": [],
   "source": [
    "response2 = tokenizer2.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CUo6gpfyK9C"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Modelo de transcrição (Whisper Open Source)\n",
    "asr_model_name = \"openai/whisper-medium\"\n",
    "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    asr_model_name, torch_dtype=torch.float16, use_safetensors=True, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(asr_model_name)\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=speech_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Modelo LLM (LLaMA 3 Instruct)\n",
    "llm_model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "def transcribe_and_summarize(audio_file):\n",
    "    try:\n",
    "        # Passo 1: Transcrição\n",
    "        result = asr_pipeline(audio_file, return_timestamps=True)\n",
    "        transcript = result[\"text\"]\n",
    "\n",
    "        # Passo 2: Construção do prompt\n",
    "        system_message = \"Você é um assistente que produz atas de reuniões a partir de transcrições, com resumo, pontos discutidos e itens de ação. O resultado deve estar em Markdown e em português.\"\n",
    "        user_prompt = f\"A seguir está uma transcrição de áudio. Gere a ata:\\n\\n{transcript}\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # Passo 3: Geração\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(inputs, max_new_tokens=1500)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return transcript, response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"Erro na transcrição\", f\"Erro durante o processamento: {str(e)}\"\n",
    "\n",
    "# Interface Gradio com duas saídas: transcrição + resumo\n",
    "interface = gr.Interface(\n",
    "    fn=transcribe_and_summarize,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Envie seu áudio (.mp3, .wav etc)\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Transcrição gerada\"),\n",
    "        gr.Markdown(label=\"Resumo em Markdown\")\n",
    "    ],\n",
    "    title=\"Resumo de Áudio com Whisper + LLaMA 3 (Colab)\",\n",
    "    description=\"Faça upload de um áudio e obtenha a transcrição + resumo automático com pontos discutidos e ações. Utiliza modelos open source. Resultados em Markdown.\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx",
     "timestamp": 1745259377448
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
